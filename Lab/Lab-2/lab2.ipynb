{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)  Implement a PySpark script that applies transformations like filter and withColumn on a \n",
    "DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---+------+\n",
      "| Names|Experience|Age|Salary|\n",
      "+------+----------+---+------+\n",
      "|Samuel|         5| 19| 10000|\n",
      "|   Kim|         6| 23| 20000|\n",
      "| Benny|         7| 25| 30000|\n",
      "| Jason|         8| 31| 40000|\n",
      "+------+----------+---+------+\n",
      "\n",
      "+-----+----------+---+------+\n",
      "|Names|Experience|Age|Salary|\n",
      "+-----+----------+---+------+\n",
      "|  Kim|         6| 23| 20000|\n",
      "|Benny|         7| 25| 30000|\n",
      "|Jason|         8| 31| 40000|\n",
      "+-----+----------+---+------+\n",
      "\n",
      "+------+----------+---+------+\n",
      "| Names|Experience|Age|Salary|\n",
      "+------+----------+---+------+\n",
      "|Samuel|         5| 19| 10000|\n",
      "| Benny|         7| 25| 30000|\n",
      "+------+----------+---+------+\n",
      "\n",
      "+-----+----------+---+------+\n",
      "|Names|Experience|Age|Salary|\n",
      "+-----+----------+---+------+\n",
      "|Benny|         7| 25| 30000|\n",
      "|Jason|         8| 31| 40000|\n",
      "+-----+----------+---+------+\n",
      "\n",
      "+------+----------+---+------+------------------------+\n",
      "| Names|Experience|Age|Salary|Experience After 2 years|\n",
      "+------+----------+---+------+------------------------+\n",
      "|Samuel|         5| 19| 10000|                       7|\n",
      "|   Kim|         6| 23| 20000|                       8|\n",
      "| Benny|         7| 25| 30000|                       9|\n",
      "| Jason|         8| 31| 40000|                      10|\n",
      "+------+----------+---+------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark \n",
    "import pandas as pd \n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.functions import col\n",
    "spark = SparkSession.builder.appName('filter').getOrCreate()\n",
    "df = pd.DataFrame({'Names':['Samuel','Kim','Benny','Jason'],\n",
    "                   'Experience':[5,6,7,8],\n",
    "                   'Age':[19,23,25,31],\n",
    "                   'Salary':[10000,20000,30000,40000]})\n",
    "df.to_csv('data.csv',index = False)\n",
    "df_pyspark = spark.read.csv('data.csv',header=True,inferSchema=True)\n",
    "df_pyspark.show()\n",
    "\n",
    "#Filter operations \n",
    "#Based on condition\n",
    "df_pyspark.filter((col(\"Age\") > 21) & (col('Salary') > 12000)).show()\n",
    "#Filtering based on name is either samuel or benny \n",
    "df_pyspark.filter(col(\"Names\").isin(['Samuel','Benny'])).show()\n",
    "#Filtering based on substring \n",
    "df_pyspark.filter(col(\"Names\").like(\"%n%\")).show()\n",
    "#withColumn operations\n",
    "df_pyspark = df_pyspark.withColumn('Experience After 2 years',df_pyspark['Experience'] + 2)\n",
    "df_pyspark.show() \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Write a PySpark script that performs actions like count and show on a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---+------+\n",
      "| Names|Experience|Age|Salary|\n",
      "+------+----------+---+------+\n",
      "|Samuel|         5| 19| 10000|\n",
      "|   Kim|         6| 23| 20000|\n",
      "| Benny|         7| 25| 30000|\n",
      "| Jason|         8| 31| 40000|\n",
      "+------+----------+---+------+\n",
      "\n",
      "No of rows in dataset: 4\n"
     ]
    }
   ],
   "source": [
    "import pyspark \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder.appName('Count').getOrCreate()\n",
    "df_pyspark = spark.read.csv('data.csv',header=True,inferSchema=True)\n",
    "df_pyspark.show()\n",
    "print(f\"No of rows in dataset: {df_pyspark.select('Names').count()}\")\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Demonstrate  how  to  perform  basic  aggregations  (e.g.,  sum,  average)  on  a  PySpark \n",
    "DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+-----------+\n",
      "|TotalExperience|AverageAge|TotalSalary|\n",
      "+---------------+----------+-----------+\n",
      "|             26|      24.5|     100000|\n",
      "+---------------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "spark = SparkSession.builder.appName('Agg').getOrCreate()\n",
    "df_pyspark = spark.read.csv('data.csv',header=True,inferSchema=True)\n",
    "df_agg = df_pyspark.agg(\n",
    "    F.sum('Experience').alias('TotalExperience'),\n",
    "    F.avg('Age').alias('AverageAge'),\n",
    "    F.sum('Salary').alias('TotalSalary')\n",
    ")\n",
    "df_agg.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Show how to write a PySpark DataFrame to a CSV file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg.write.csv('agg_data.csv',header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Implement wordcount program in PySpark. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/12 15:24:16 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Count Results:\n",
      "+---------------+-----+\n",
      "|           word|count|\n",
      "+---------------+-----+\n",
      "|          eggs,|    1|\n",
      "|           used|    1|\n",
      "|           even|    1|\n",
      "|        bananas|    1|\n",
      "|           meat|    1|\n",
      "|        species|    1|\n",
      "|black-and-white|    1|\n",
      "|           name|    1|\n",
      "|         panda\"|    1|\n",
      "|     captivity,|    1|\n",
      "|    food.[6][7]|    1|\n",
      "|           form|    1|\n",
      "|             by|    1|\n",
      "|           more|    1|\n",
      "|             In|    1|\n",
      "|          panda|    3|\n",
      "|  characterised|    1|\n",
      "|            eat|    1|\n",
      "|          along|    1|\n",
      "|         bamboo|    1|\n",
      "+---------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, split\n",
    "spark = SparkSession.builder.appName('wordcount').getOrCreate()\n",
    "data = '/home/lplab/Desktop/Jayasuryan_BDA/text'\n",
    "df = spark.read.text(data)\n",
    "words_df = df.select(explode(split(df.value, ' ')).alias('word'))\n",
    "word_count_df = words_df.groupBy('word').count()\n",
    "print(\"Word Count Results:\")\n",
    "word_count_df.show()\n",
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
